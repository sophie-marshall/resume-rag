{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ResumeRAG` Pipeline Testing\n",
    "\n",
    "**Goal:** Test individual pipeline components here before scriptifying them and eventually turning this into an API?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data\n",
    "\n",
    "For this example, all data is coming from a Google Doc file that can be accessed using a GoogleAPI Service Account and the associated Python API client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/srmarshall/Desktop/code/personal/resume-rag/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from utils.google import GoogleDocClient\n",
    "\n",
    "# instantiate a client\n",
    "docs_client = GoogleDocClient(\n",
    "    service_account_json=\"../credentials.json\", \n",
    "    scopes=['https://www.googleapis.com/auth/documents.readonly']\n",
    ")\n",
    "\n",
    "# fetch document\n",
    "response = docs_client.fetch_document(document_id=os.getenv(\"RESUME_RAG_DOCUMENT_ID\"))\n",
    "\n",
    "# extract text \n",
    "raw_text = docs_client.extract_text(google_doc_repsonse=response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up our raw text a bit to prepare for our embedings step. We'll remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import strip_text\n",
    "\n",
    "# strip our documents raw text\n",
    "clean_text = strip_text(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, well split our large document into workable chunks. Chunking our text not only improves the accuracy, but also ensures we wont bump up against any token limist when we go to embed our content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# instantiate text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "\n",
    "# split texts \n",
    "split_texts = text_splitter.split_text(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've cleaned and split our text, we're ready to embed. There are a host of embedding models avaiable for use (even multi-modal ones if you'd like to include non text documents in your knowledge base). For this project, we'll use `MiniLM-L6-v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# instantiate the model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# generate embeddings using the model\n",
    "embeddings = model.encode(split_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
