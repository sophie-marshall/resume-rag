{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ResumeRAG` Pipeline Testing\n",
    "\n",
    "**Goal:** Test individual pipeline components here before scriptifying them and eventually turning this into an API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data\n",
    "\n",
    "For this example, we will pull our text data from a Google Doc. We can programmatically access this file using a GoogleAPI Service Account and the avaiable Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/srmarshall/Desktop/code/personal/resume-rag/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from utils.google import GoogleDocClient\n",
    "\n",
    "# instantiate a client\n",
    "docs_client = GoogleDocClient(\n",
    "    service_account_json=\"../credentials.json\", \n",
    "    scopes=['https://www.googleapis.com/auth/documents.readonly']\n",
    ")\n",
    "\n",
    "# fetch document\n",
    "response = docs_client.fetch_document(document_id=os.getenv(\"RESUME_RAG_DOCUMENT_ID\"))\n",
    "\n",
    "# extract text \n",
    "raw_text = docs_client.extract_text(google_doc_repsonse=response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Data\n",
    "\n",
    "Our vector database has the following columns `document_id`, `chunk_id`, `tags`, `clean_text`, and `embedding`. We will need to generate content to match each of these fiels in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import strip_text\n",
    "\n",
    "# strip our documents raw text to clean it up a bit and ensure uniformity\n",
    "clean_text = strip_text(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, well split our large document into workable chunks. Chunking our text not only improves the accuracy/relevance of information returned by our retrieval mechanism, but also ensures we won't bump up against any token limist when we go to embed our content. \n",
    "\n",
    "Each split text will represent a row in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# instantiate text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "\n",
    "# split texts \n",
    "split_texts = text_splitter.split_text(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've cleaned and split our text, we're ready to embed. There are a host of embedding models avaiable for use (even multi-modal ones if you'd like to include non text documents in your knowledge base). For this project, we'll use `MiniLM-L6-v2` which is free to access using the `SentenceTransformer` library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srmarshall/.virtualenvs/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# instantiate the model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# generate content embeddings for each chunk of split text\n",
    "embeddings = model.encode(split_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to stich our content together to create full rows for our database. We'll create a list of dictionaries to iterate over and insert into Postgres. Each dictionary represents a row where the `key` is the column name and the `value` is the columns value for the specified row:\n",
    "- `document_id` comes from Google Drive and will enbale document reconstruction in the future \n",
    "- `chunk_id` is generated by us will also play a role in document reconstruction. Think \"for each `document_id` grab all rows then order by `chunk_id`\" to reconstruct the full document\n",
    "- `tags` are also generated by us, and can be used to enhance our retrieval process\n",
    "- `clean_text` is a single text chunk generated by the text splitter above\n",
    "- `embedding` is the vector representation of the `clean_text` field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a list to hold transformed data\n",
    "transformed_data = []\n",
    "\n",
    "# populate list\n",
    "for index, text in enumerate(split_texts):\n",
    "\n",
    "    # instantiate single record\n",
    "    record = {}\n",
    "\n",
    "    # populate dict for this text\n",
    "    record[\"document_id\"] = os.getenv(\"RESUME_RAG_DOCUMENT_ID\") \n",
    "    record[\"chunk_id\"] = index \n",
    "    record[\"tags\"] = \"resume\" \n",
    "    record[\"clean_text\"] = text\n",
    "    record[\"embedding\"] = embeddings[index]\n",
    "    \n",
    "    # add to master list \n",
    "    transformed_data.append(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "The final step is iterating over our list of dictionaries and adding them to our Postgres database. Once we verify our data is available in Postgres we're ready to start querying!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.database import PgClient\n",
    "\n",
    "# instantiate client\n",
    "pg_client = PgClient(\n",
    "    pg_host = os.getenv(\"PG_HOST\"), \n",
    "    pg_user = os.getenv(\"PG_USER\"), \n",
    "    pg_password = os.getenv(\"PG_PASSWORD\"), \n",
    "    pg_db = \"resume_rag\"\n",
    ")\n",
    "\n",
    "# insert data\n",
    "pg_client.insert_content_embeddings(transformed_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
